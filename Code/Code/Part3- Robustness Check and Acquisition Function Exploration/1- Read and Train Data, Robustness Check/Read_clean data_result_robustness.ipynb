{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lijing/Desktop/code/4742/PersonalProject_OurBranch/WM-SecuritySelection\n"
     ]
    }
   ],
   "source": [
    "cd \"/Users/lijing/Desktop/code/4742/PersonalProject_OurBranch/WM-SecuritySelection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from clean_data import ReadData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from train_data import TrainDataLearning\n",
    "from train_data import TrainDataLearningBatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1330/1330 [02:11<00:00, 10.09it/s]\n",
      "100%|██████████| 1130/1130 [00:07<00:00, 160.46it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\": \n",
    "    filepath_1 = \"/Users/lijing/Desktop/code/4742/PersonalProject_OurBranch/WM-SecuritySelection/security_selection/automation/notebooks/MF_LargeCap_ExcessReturn_3Y.parquet\"\n",
    "    Data=ReadData(filepath_1)\n",
    "    data_dict, label_dict=Data.prepare_data_for_er_ari()\n",
    "    x_train, x_val, y_train, y_val=Data.train_test_data(data_dict,label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "424/424 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "424/424 [==============================] - 3s 6ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 8/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 9/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6463e-04\n",
      "Epoch 10/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9846e-04\n",
      "Epoch 11/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.4559e-04\n",
      "Epoch 12/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.5662e-04\n",
      "Epoch 13/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3999e-04\n",
      "Epoch 14/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6343e-04\n",
      "Epoch 15/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9140e-04\n",
      "Epoch 16/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.9633e-04 - val_loss: 9.3585e-04\n",
      "Epoch 17/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.8114e-04 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7821e-04 - val_loss: 9.1244e-04\n",
      "Epoch 19/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7778e-04 - val_loss: 9.7248e-04\n",
      "Epoch 20/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7747e-04 - val_loss: 9.0992e-04\n",
      "Epoch 21/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.6640e-04 - val_loss: 9.1509e-04\n",
      "Epoch 22/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.5682e-04 - val_loss: 9.1543e-04\n",
      "Epoch 23/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.8093e-04 - val_loss: 9.9280e-04\n",
      "Epoch 24/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7200e-04 - val_loss: 9.1743e-04\n",
      "Epoch 25/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.5164e-04 - val_loss: 9.1012e-04\n",
      "Epoch 26/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.6026e-04 - val_loss: 9.3645e-04\n",
      "Epoch 27/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.6285e-04 - val_loss: 9.6862e-04\n",
      "Epoch 28/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.5771e-04 - val_loss: 9.4839e-04\n",
      "Epoch 29/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.4680e-04 - val_loss: 0.0010\n",
      "Epoch 30/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.4600e-04 - val_loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "596/596 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.6075e-04\n",
      "Epoch 5/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 9.7884e-04\n",
      "Epoch 6/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.6010e-04\n",
      "Epoch 8/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.6602e-04\n",
      "Epoch 9/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.8939e-04\n",
      "Epoch 10/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.9759e-04 - val_loss: 9.4053e-04\n",
      "Epoch 11/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.3558e-04\n",
      "Epoch 12/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.7568e-04 - val_loss: 8.5170e-04\n",
      "Epoch 13/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.6349e-04 - val_loss: 9.3642e-04\n",
      "Epoch 14/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.7696e-04 - val_loss: 9.0866e-04\n",
      "Epoch 15/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.8573e-04 - val_loss: 9.7137e-04\n",
      "Epoch 16/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.6049e-04 - val_loss: 8.2379e-04\n",
      "Epoch 17/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 9.5259e-04 - val_loss: 8.9787e-04\n",
      "Epoch 18/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.2863e-04 - val_loss: 9.3864e-04\n",
      "Epoch 19/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.2848e-04 - val_loss: 7.9041e-04\n",
      "Epoch 20/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 8.8341e-04 - val_loss: 8.0803e-04\n",
      "Epoch 21/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.1385e-04 - val_loss: 8.1275e-04\n",
      "Epoch 22/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 8.8716e-04 - val_loss: 7.4930e-04\n",
      "Epoch 23/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 8.9391e-04 - val_loss: 7.7246e-04\n",
      "Epoch 24/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 8.9191e-04 - val_loss: 7.7455e-04\n",
      "Epoch 25/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 8.8357e-04 - val_loss: 7.6258e-04\n",
      "Epoch 26/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.1329e-04 - val_loss: 9.1930e-04\n",
      "Epoch 27/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.5031e-04 - val_loss: 8.8859e-04\n",
      "Epoch 28/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.4550e-04 - val_loss: 9.5016e-04\n",
      "Epoch 29/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.4544e-04 - val_loss: 9.0191e-04\n",
      "Epoch 30/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 9.3025e-04 - val_loss: 8.8979e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "523/523 [==============================] - 7s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4370e-04\n",
      "Epoch 6/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.3471e-04\n",
      "Epoch 8/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.7191e-04\n",
      "Epoch 10/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.2672e-04\n",
      "Epoch 11/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.5644e-04\n",
      "Epoch 12/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.7541e-04 - val_loss: 9.1959e-04\n",
      "Epoch 13/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5211e-04 - val_loss: 9.6177e-04\n",
      "Epoch 14/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.4609e-04 - val_loss: 8.4854e-04\n",
      "Epoch 15/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5414e-04 - val_loss: 9.7380e-04\n",
      "Epoch 16/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.8782e-04 - val_loss: 8.8687e-04\n",
      "Epoch 17/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5638e-04 - val_loss: 9.1384e-04\n",
      "Epoch 18/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.4355e-04 - val_loss: 8.7061e-04\n",
      "Epoch 19/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.4820e-04 - val_loss: 0.0010\n",
      "Epoch 20/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.3868e-04 - val_loss: 7.7710e-04\n",
      "Epoch 21/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2792e-04 - val_loss: 7.9401e-04\n",
      "Epoch 22/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.1776e-04 - val_loss: 7.7167e-04\n",
      "Epoch 23/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2146e-04 - val_loss: 9.0688e-04\n",
      "Epoch 24/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.9109e-04 - val_loss: 9.0104e-04\n",
      "Epoch 25/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.8240e-04 - val_loss: 7.3748e-04\n",
      "Epoch 26/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.8659e-04 - val_loss: 7.6604e-04\n",
      "Epoch 27/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.9292e-04 - val_loss: 7.4177e-04\n",
      "Epoch 28/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.8561e-04 - val_loss: 7.8352e-04\n",
      "Epoch 29/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.7337e-04 - val_loss: 7.8571e-04\n",
      "Epoch 30/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 8.4603e-04 - val_loss: 7.5419e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "509/509 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 8/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "509/509 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "509/509 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 7s 8ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.7054e-04\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.0619e-04\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.5469e-04\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.0043e-04\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.9983e-04 - val_loss: 8.6545e-04\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.6468e-04 - val_loss: 8.5307e-04\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8928e-04 - val_loss: 8.9875e-04\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8755e-04 - val_loss: 9.6085e-04\n",
      "Epoch 15/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 8.6840e-04\n",
      "Epoch 16/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8116e-04 - val_loss: 8.8265e-04\n",
      "Epoch 17/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.9163e-04\n",
      "Epoch 18/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 19/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 20/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 21/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.7071e-04\n",
      "Epoch 22/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.8739e-04\n",
      "Epoch 23/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.2139e-04\n",
      "Epoch 24/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.5253e-04\n",
      "Epoch 25/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8683e-04 - val_loss: 9.7657e-04\n",
      "Epoch 26/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8340e-04 - val_loss: 9.0627e-04\n",
      "Epoch 27/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8123e-04 - val_loss: 8.9116e-04\n",
      "Epoch 28/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.7772e-04 - val_loss: 0.0010\n",
      "Epoch 29/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.8642e-04 - val_loss: 0.0010\n",
      "Epoch 30/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.6592e-04 - val_loss: 9.3465e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "530/530 [==============================] - 7s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 8/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "530/530 [==============================] - 3s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 28/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "424/424 [==============================] - 6s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 5/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "523/523 [==============================] - 7s 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 4/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0093\n",
      "Epoch 8/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 9/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 11/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 23/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 26/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "553/553 [==============================] - 7s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 2/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 6/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 11/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 15/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 16/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 17/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 18/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 19/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 20/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 21/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 22/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 23/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 25/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 26/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 27/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 28/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 29/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 30/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 7s 9ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.0010 - val_loss: 9.4774e-04\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.6956e-04\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.7907e-04\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9377e-04\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 9.8603e-04 - val_loss: 9.1091e-04\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 9.6479e-04 - val_loss: 8.3590e-04\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 9.4194e-04 - val_loss: 8.3310e-04\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 9.4056e-04 - val_loss: 8.7587e-04\n",
      "Epoch 15/30\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 9.2309e-04 - val_loss: 7.9740e-04\n",
      "Epoch 16/30\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 9.1861e-04 - val_loss: 8.1460e-04\n",
      "Epoch 17/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.0679e-04 - val_loss: 7.5348e-04\n",
      "Epoch 18/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.9980e-04 - val_loss: 7.5711e-04\n",
      "Epoch 19/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 9.0166e-04 - val_loss: 7.6784e-04\n",
      "Epoch 20/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.9122e-04 - val_loss: 7.6437e-04\n",
      "Epoch 21/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.7959e-04 - val_loss: 7.5592e-04\n",
      "Epoch 22/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.7145e-04 - val_loss: 8.6948e-04\n",
      "Epoch 23/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.5363e-04 - val_loss: 7.2794e-04\n",
      "Epoch 24/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.6039e-04 - val_loss: 7.6661e-04\n",
      "Epoch 25/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.5594e-04 - val_loss: 6.9928e-04\n",
      "Epoch 26/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.4929e-04 - val_loss: 6.8567e-04\n",
      "Epoch 27/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.4621e-04 - val_loss: 6.9217e-04\n",
      "Epoch 28/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.3442e-04 - val_loss: 7.3091e-04\n",
      "Epoch 29/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.1618e-04 - val_loss: 7.6808e-04\n",
      "Epoch 30/30\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 8.3367e-04 - val_loss: 7.0200e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "318/318 [==============================] - 6s 8ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.4501e-04\n",
      "Epoch 6/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.9879e-04 - val_loss: 9.5950e-04\n",
      "Epoch 7/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 8/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.9497e-04 - val_loss: 8.9388e-04\n",
      "Epoch 10/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.7130e-04 - val_loss: 8.9365e-04\n",
      "Epoch 11/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.6736e-04 - val_loss: 9.1118e-04\n",
      "Epoch 12/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.3759e-04 - val_loss: 8.7864e-04\n",
      "Epoch 13/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.3649e-04 - val_loss: 8.9460e-04\n",
      "Epoch 14/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.5202e-04 - val_loss: 8.3978e-04\n",
      "Epoch 15/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.3171e-04 - val_loss: 8.3125e-04\n",
      "Epoch 16/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.2710e-04 - val_loss: 8.2640e-04\n",
      "Epoch 17/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.4653e-04 - val_loss: 8.6361e-04\n",
      "Epoch 18/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.0738e-04 - val_loss: 8.7833e-04\n",
      "Epoch 19/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.0208e-04 - val_loss: 8.3264e-04\n",
      "Epoch 20/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 8.9195e-04 - val_loss: 7.7303e-04\n",
      "Epoch 21/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 8.7175e-04 - val_loss: 8.4905e-04\n",
      "Epoch 22/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 8.6950e-04 - val_loss: 7.8510e-04\n",
      "Epoch 23/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.2064e-04 - val_loss: 9.6410e-04\n",
      "Epoch 24/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.4172e-04 - val_loss: 9.6672e-04\n",
      "Epoch 25/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.5903e-04 - val_loss: 8.9572e-04\n",
      "Epoch 26/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.1856e-04 - val_loss: 9.2226e-04\n",
      "Epoch 27/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.1637e-04 - val_loss: 8.4799e-04\n",
      "Epoch 28/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 9.0300e-04 - val_loss: 9.1619e-04\n",
      "Epoch 29/30\n",
      "318/318 [==============================] - 2s 5ms/step - loss: 9.0078e-04 - val_loss: 8.7771e-04\n",
      "Epoch 30/30\n",
      "318/318 [==============================] - 2s 6ms/step - loss: 8.9054e-04 - val_loss: 7.7352e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "419/419 [==============================] - 6s 7ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.7339e-04\n",
      "Epoch 7/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.1629e-04\n",
      "Epoch 8/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9459e-04\n",
      "Epoch 9/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.8344e-04 - val_loss: 8.4066e-04\n",
      "Epoch 10/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.7640e-04 - val_loss: 9.3371e-04\n",
      "Epoch 11/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.9391e-04 - val_loss: 9.0978e-04\n",
      "Epoch 12/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.7834e-04 - val_loss: 9.5810e-04\n",
      "Epoch 13/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.4032e-04 - val_loss: 9.3582e-04\n",
      "Epoch 14/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.5763e-04 - val_loss: 0.0010\n",
      "Epoch 15/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6640e-04\n",
      "Epoch 16/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.9953e-04 - val_loss: 9.7948e-04\n",
      "Epoch 17/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.8514e-04 - val_loss: 9.2060e-04\n",
      "Epoch 18/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.9352e-04 - val_loss: 9.2292e-04\n",
      "Epoch 19/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.7600e-04 - val_loss: 9.0323e-04\n",
      "Epoch 20/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.6037e-04 - val_loss: 9.8951e-04\n",
      "Epoch 21/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.6912e-04 - val_loss: 9.0885e-04\n",
      "Epoch 22/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.4092e-04 - val_loss: 9.2002e-04\n",
      "Epoch 23/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.4203e-04 - val_loss: 9.3325e-04\n",
      "Epoch 24/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.3041e-04 - val_loss: 9.4670e-04\n",
      "Epoch 25/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.3283e-04 - val_loss: 9.0899e-04\n",
      "Epoch 26/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.2598e-04 - val_loss: 8.9832e-04\n",
      "Epoch 27/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.3335e-04 - val_loss: 0.0011\n",
      "Epoch 28/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.1204e-04 - val_loss: 8.6239e-04\n",
      "Epoch 29/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.0889e-04 - val_loss: 8.6497e-04\n",
      "Epoch 30/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 9.0490e-04 - val_loss: 8.4446e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "596/596 [==============================] - 7s 6ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 5/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 9.9050e-04\n",
      "Epoch 6/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.9393e-04\n",
      "Epoch 7/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 9.6804e-04\n",
      "Epoch 8/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 9/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 9.9970e-04\n",
      "Epoch 10/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 11/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 9.9309e-04\n",
      "Epoch 12/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0010 - val_loss: 9.0501e-04\n",
      "Epoch 13/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 15/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 17/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 19/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 20/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 21/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 22/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 23/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8639e-04\n",
      "Epoch 25/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 26/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 27/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 28/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 29/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 30/30\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0011 - val_loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "385/385 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 4/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 6/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.8871e-04\n",
      "Epoch 7/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.7847e-04\n",
      "Epoch 8/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 9/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 11/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.5393e-04\n",
      "Epoch 12/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 14/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 17/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 19/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 20/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 22/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 23/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 25/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 26/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 27/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 28/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 29/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 30/30\n",
      "385/385 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.4873e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "523/523 [==============================] - 7s 6ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.7155e-04\n",
      "Epoch 6/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.4587e-04\n",
      "Epoch 7/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4057e-04\n",
      "Epoch 8/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 8.7490e-04\n",
      "Epoch 9/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 8.7295e-04\n",
      "Epoch 10/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.6874e-04 - val_loss: 8.7690e-04\n",
      "Epoch 11/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.6598e-04 - val_loss: 8.7012e-04\n",
      "Epoch 12/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5412e-04 - val_loss: 7.5010e-04\n",
      "Epoch 13/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2948e-04 - val_loss: 8.8108e-04\n",
      "Epoch 14/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.6335e-04 - val_loss: 9.9805e-04\n",
      "Epoch 15/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.6148e-04 - val_loss: 7.3153e-04\n",
      "Epoch 16/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.4680e-04 - val_loss: 9.6046e-04\n",
      "Epoch 17/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5269e-04 - val_loss: 7.9332e-04\n",
      "Epoch 18/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.5047e-04 - val_loss: 7.3445e-04\n",
      "Epoch 19/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.1383e-04 - val_loss: 8.5093e-04\n",
      "Epoch 20/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2844e-04 - val_loss: 7.4610e-04\n",
      "Epoch 21/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2111e-04 - val_loss: 8.0262e-04\n",
      "Epoch 22/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.0236e-04 - val_loss: 8.3283e-04\n",
      "Epoch 23/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.2024e-04 - val_loss: 7.4317e-04\n",
      "Epoch 24/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.7813e-04 - val_loss: 0.0010\n",
      "Epoch 25/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.9383e-04 - val_loss: 9.2953e-04\n",
      "Epoch 26/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.9209e-04 - val_loss: 9.5293e-04\n",
      "Epoch 27/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.8314e-04 - val_loss: 9.2393e-04\n",
      "Epoch 28/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.7780e-04 - val_loss: 9.2612e-04\n",
      "Epoch 29/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.8571e-04 - val_loss: 9.9282e-04\n",
      "Epoch 30/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 9.8067e-04 - val_loss: 9.2348e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "561/561 [==============================] - 7s 6ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.6085e-04\n",
      "Epoch 5/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.5138e-04\n",
      "Epoch 6/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.5654e-04\n",
      "Epoch 7/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 12/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 14/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 17/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8249e-04\n",
      "Epoch 19/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 20/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8823e-04\n",
      "Epoch 21/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8277e-04\n",
      "Epoch 22/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 23/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.7967e-04\n",
      "Epoch 25/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.5534e-04\n",
      "Epoch 26/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 27/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.2175e-04\n",
      "Epoch 28/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 29/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4995e-04\n",
      "Epoch 30/30\n",
      "561/561 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.3906e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "449/449 [==============================] - 6s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.8923e-04\n",
      "Epoch 5/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6688e-04\n",
      "Epoch 6/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 7/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3988e-04\n",
      "Epoch 8/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 8.5219e-04\n",
      "Epoch 9/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.8480e-04\n",
      "Epoch 11/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 13/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.7803e-04\n",
      "Epoch 14/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 8.9576e-04\n",
      "Epoch 15/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 16/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6436e-04\n",
      "Epoch 17/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.2523e-04\n",
      "Epoch 19/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6431e-04\n",
      "Epoch 20/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.8471e-04 - val_loss: 9.8916e-04\n",
      "Epoch 21/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.5258e-04 - val_loss: 8.5070e-04\n",
      "Epoch 22/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.1840e-04 - val_loss: 7.6394e-04\n",
      "Epoch 23/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.5592e-04 - val_loss: 9.6907e-04\n",
      "Epoch 24/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.2910e-04\n",
      "Epoch 25/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.9644e-04 - val_loss: 9.1650e-04\n",
      "Epoch 26/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.9068e-04 - val_loss: 0.0011\n",
      "Epoch 27/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.8641e-04 - val_loss: 9.8102e-04\n",
      "Epoch 28/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.7901e-04 - val_loss: 9.7431e-04\n",
      "Epoch 29/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.9284e-04 - val_loss: 9.4798e-04\n",
      "Epoch 30/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.7054e-04 - val_loss: 9.0756e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "502/502 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 6/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "502/502 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 11/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "502/502 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 27/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "502/502 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "313/313 [==============================] - 6s 8ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.7327e-04\n",
      "Epoch 6/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 9.4367e-04\n",
      "Epoch 8/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9801e-04\n",
      "Epoch 9/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.9953e-04 - val_loss: 8.6423e-04\n",
      "Epoch 10/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.7078e-04 - val_loss: 9.0491e-04\n",
      "Epoch 11/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.9246e-04 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.2921e-04\n",
      "Epoch 13/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.9840e-04 - val_loss: 9.6216e-04\n",
      "Epoch 14/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.7281e-04 - val_loss: 9.0527e-04\n",
      "Epoch 15/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.4898e-04 - val_loss: 8.5456e-04\n",
      "Epoch 16/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.6468e-04 - val_loss: 8.9269e-04\n",
      "Epoch 17/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.7437e-04\n",
      "Epoch 19/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 9.6778e-04\n",
      "Epoch 20/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.8847e-04 - val_loss: 0.0010\n",
      "Epoch 21/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.8384e-04 - val_loss: 9.5157e-04\n",
      "Epoch 22/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.8829e-04 - val_loss: 9.0304e-04\n",
      "Epoch 23/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.7821e-04 - val_loss: 9.5512e-04\n",
      "Epoch 24/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.7277e-04 - val_loss: 9.7058e-04\n",
      "Epoch 25/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.5149e-04 - val_loss: 8.9260e-04\n",
      "Epoch 26/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.5359e-04 - val_loss: 8.8312e-04\n",
      "Epoch 27/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.5330e-04 - val_loss: 9.1292e-04\n",
      "Epoch 28/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.4594e-04 - val_loss: 9.5419e-04\n",
      "Epoch 29/30\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.3725e-04 - val_loss: 0.0010\n",
      "Epoch 30/30\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 9.4686e-04 - val_loss: 8.7747e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "516/516 [==============================] - 7s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 8/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.7094e-04\n",
      "Epoch 9/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 10/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 11/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4263e-04\n",
      "Epoch 12/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4162e-04\n",
      "Epoch 14/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 15/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 18/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 25/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "350/350 [==============================] - 6s 8ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "350/350 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "350/350 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "350/350 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "350/350 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "350/350 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "350/350 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "553/553 [==============================] - 7s 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 6/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 15/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 19/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 21/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "449/449 [==============================] - 7s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 2/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.6941e-04\n",
      "Epoch 5/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.9360e-04\n",
      "Epoch 9/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3242e-04\n",
      "Epoch 10/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 8.8219e-04\n",
      "Epoch 11/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6833e-04\n",
      "Epoch 12/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3547e-04\n",
      "Epoch 13/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3466e-04\n",
      "Epoch 14/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 15/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.3595e-04\n",
      "Epoch 16/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 8.5135e-04\n",
      "Epoch 17/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.6122e-04 - val_loss: 8.0583e-04\n",
      "Epoch 18/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.6145e-04 - val_loss: 8.0367e-04\n",
      "Epoch 19/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.3981e-04 - val_loss: 9.0586e-04\n",
      "Epoch 20/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.3908e-04 - val_loss: 7.7366e-04\n",
      "Epoch 21/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.1446e-04 - val_loss: 7.3464e-04\n",
      "Epoch 22/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.9796e-04 - val_loss: 7.6096e-04\n",
      "Epoch 23/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.2346e-04 - val_loss: 7.0410e-04\n",
      "Epoch 24/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.9165e-04 - val_loss: 7.9682e-04\n",
      "Epoch 25/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.9847e-04 - val_loss: 8.4671e-04\n",
      "Epoch 26/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.8471e-04 - val_loss: 7.1540e-04\n",
      "Epoch 27/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 9.0094e-04 - val_loss: 7.0976e-04\n",
      "Epoch 28/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.6982e-04 - val_loss: 7.1664e-04\n",
      "Epoch 29/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.6701e-04 - val_loss: 7.1433e-04\n",
      "Epoch 30/30\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 8.5960e-04 - val_loss: 7.0671e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "397/397 [==============================] - 7s 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 3/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 4/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 5/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 6/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 11/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      "397/397 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "419/419 [==============================] - 6s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "419/419 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "335/335 [==============================] - 6s 8ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 27/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "335/335 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "523/523 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 4/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 8/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 11/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 18/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 19/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "410/410 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 22/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "410/410 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "402/402 [==============================] - 7s 8ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 24/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "424/424 [==============================] - 6s 7ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.5247e-04\n",
      "Epoch 5/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 8/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.7283e-04\n",
      "Epoch 9/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.8757e-04\n",
      "Epoch 10/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 9.5826e-04\n",
      "Epoch 11/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 12/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.5963e-04\n",
      "Epoch 13/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6413e-04\n",
      "Epoch 14/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 15/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.6718e-04\n",
      "Epoch 16/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.3148e-04\n",
      "Epoch 17/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.5402e-04\n",
      "Epoch 18/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9618e-04\n",
      "Epoch 19/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.9951e-04\n",
      "Epoch 20/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 21/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.9699e-04 - val_loss: 9.0781e-04\n",
      "Epoch 22/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 9.4547e-04\n",
      "Epoch 23/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.9568e-04 - val_loss: 9.6008e-04\n",
      "Epoch 24/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.8139e-04 - val_loss: 9.2774e-04\n",
      "Epoch 25/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.9177e-04 - val_loss: 9.0177e-04\n",
      "Epoch 26/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7706e-04 - val_loss: 9.3140e-04\n",
      "Epoch 27/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7353e-04 - val_loss: 9.2251e-04\n",
      "Epoch 28/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.7716e-04 - val_loss: 9.0609e-04\n",
      "Epoch 29/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.5970e-04 - val_loss: 9.4889e-04\n",
      "Epoch 30/30\n",
      "424/424 [==============================] - 2s 5ms/step - loss: 9.6582e-04 - val_loss: 9.6674e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "523/523 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 4/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/30\n",
      "523/523 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "587/587 [==============================] - 7s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.9415e-04\n",
      "Epoch 4/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4500e-04\n",
      "Epoch 8/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.8553e-04\n",
      "Epoch 9/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.9467e-04 - val_loss: 9.1819e-04\n",
      "Epoch 10/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.8973e-04 - val_loss: 9.6889e-04\n",
      "Epoch 11/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.5446e-04 - val_loss: 8.9809e-04\n",
      "Epoch 12/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.4307e-04 - val_loss: 8.1910e-04\n",
      "Epoch 13/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.6836e-04 - val_loss: 0.0013\n",
      "Epoch 14/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.2601e-04\n",
      "Epoch 15/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.9295e-04 - val_loss: 9.5112e-04\n",
      "Epoch 16/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.8166e-04 - val_loss: 9.2203e-04\n",
      "Epoch 17/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.6696e-04 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.7056e-04 - val_loss: 9.3020e-04\n",
      "Epoch 19/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.5268e-04 - val_loss: 9.3971e-04\n",
      "Epoch 20/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.5467e-04 - val_loss: 8.9498e-04\n",
      "Epoch 21/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.4680e-04 - val_loss: 8.9955e-04\n",
      "Epoch 22/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.4916e-04 - val_loss: 8.6043e-04\n",
      "Epoch 23/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.2777e-04 - val_loss: 9.6372e-04\n",
      "Epoch 24/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.3255e-04 - val_loss: 8.8908e-04\n",
      "Epoch 25/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.2397e-04 - val_loss: 8.4822e-04\n",
      "Epoch 26/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.2528e-04 - val_loss: 9.5160e-04\n",
      "Epoch 27/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.1220e-04 - val_loss: 9.0987e-04\n",
      "Epoch 28/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.0091e-04 - val_loss: 8.7931e-04\n",
      "Epoch 29/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 9.2646e-04 - val_loss: 8.4781e-04\n",
      "Epoch 30/30\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 8.9858e-04 - val_loss: 9.0088e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "569/569 [==============================] - 7s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 2/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 6/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8962e-04\n",
      "Epoch 7/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.7884e-04\n",
      "Epoch 8/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.4477e-04\n",
      "Epoch 9/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 8.6970e-04\n",
      "Epoch 10/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.8828e-04\n",
      "Epoch 12/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.5534e-04\n",
      "Epoch 14/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 15/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.9503e-04\n",
      "Epoch 16/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 17/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 19/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 20/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 21/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 22/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 23/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.3917e-04\n",
      "Epoch 25/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 26/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 27/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 9.8001e-04\n",
      "Epoch 28/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 29/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.7544e-04\n",
      "Epoch 30/30\n",
      "569/569 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 9.7434e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 128)         84992     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,873\n",
      "Trainable params: 175,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "553/553 [==============================] - 7s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 4/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 7/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 17/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 19/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 23/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 26/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 28/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 30/30\n",
      "553/553 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "{'target': 37.414695292252944, 'params': {'batch_size': 85.7089961305027, 'learning_rate': 0.012339482024669689}}\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "train_intial=TrainDataLearningBatch(x_train, x_val, y_train, y_val,num_epochs=30)\n",
    "\n",
    "optimizer =BayesianOptimization(f=train_intial.train_learning,\n",
    "    pbounds={'learning_rate': (0.01, 0.02), \"batch_size\":(64,128)},  # \"num_epochs\":(70,100)},\n",
    "        #early_stop_fn=no_progress_loss(iteration_stop_count=10,percent_increase=0.0),\n",
    "\n",
    "    verbose=0,\n",
    "    random_state=1)\n",
    "    \n",
    "optimizer.maximize(n_iter=30)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the maximum of the inverse of the RMSE with batch_size=86 and learning_rate=0.01234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intial=TrainDataLearning(x_train, x_val, y_train, y_val)\n",
    "\n",
    "x = np.arange(0.001, 0.05, 0.005)\n",
    "\n",
    "object_one=[]\n",
    "object_mean=[]\n",
    "for i in range(len(x)):\n",
    "    object_one.append(train_intial.train_learning(x[i]))\n",
    "    object_mean.append(np.mean([train_intial.train_learning(x[i]) for t in range(5)]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, object_one, color='blue', label='raw')\n",
    "\n",
    "plt.xlabel(\"learning_rate\")\n",
    "plt.ylabel(\"inverse of RMSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,object_mean, color=\"red\",label=\"mean\")\n",
    "plt.xlabel(\"learning_rate\")\n",
    "plt.ylabel(\"inverse of RMSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83a8f8fdd9a4bfee2adbfced20061e3a8205137c55f327fc17073af9bf339e94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
