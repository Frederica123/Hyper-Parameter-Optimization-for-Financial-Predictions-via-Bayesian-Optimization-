{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction, Data Processing, Algorithm Demonstration and Key Hyperparameters\n",
    "\n",
    "Yufei Jin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Optimization (HPO) is an important part of AutoML, where we try to search for the “best” set of hyper-parameters for an ML/DL model. Common methods include grid search and random search. \n",
    "\n",
    "### However, each time we want to examine the performance (e.g. prediction accuracy) of certain sets of hyper-parameters, we need to train the model from the very beginning, which could be extremely time-consuming along with high cost of computer resources. In other words, we have very limited data samples. \n",
    "\n",
    "## To deal with this issue, we introduce Bayesian Optimization (BO) which is famous for being data-efficient and has been a powerful tool for HPO. Our task is to use Bayesian Optimization to search for the “best” hyper-parameters for the time series forecasting models (specificly LSTM model in our case) used in the wealth management project. The metric of success is the hyperparameters found by BO beat the performance of the default hyperparameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederica\\Desktop\\msor semester 1\\WM-SecuritySelection\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\Frederica\\Desktop\\msor semester 1\\WM-SecuritySelection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All of import stuff that we used in this project\n",
    "import sys \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import pickle # used to save serialized files\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import security_selection.configuration as conf\n",
    "from utils import read_json, write_json\n",
    "\n",
    "from data.main import Pool, Meta\n",
    "from data.reader.helpers import read_reader_params_from_json, read_performance_measures_params_from_json\n",
    "\n",
    "from security_selection.feature.main import Feature\n",
    "from security_selection.feature.performance_measures import *\n",
    "\n",
    "from security_selection.training.main import TrainingEnv\n",
    "\n",
    "from security_selection.model.base import EstimatorBased, TransformerBased\n",
    "from security_selection.model.sequence_model import LSTM, sequence_model_input_shape_map, sequence_model_single_dim_instance_prediction_input_shape_map\n",
    "\n",
    "from datetime import datetime\n",
    "from security_selection.training.helpers import *\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to prepare the data that we are going to train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"C:/Users/Frederica/Desktop/msor semester 1/WM-SecuritySelection/security_selection/automation/notebooks/MF_LargeCap_ExcessReturn_3Y.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is our settings of parameters for data processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_window = 90\n",
    "\n",
    "# performance measures window: number of years\n",
    "pm_window = 3\n",
    "lb_window = int(3 * pm_window * 365.25) + 1\n",
    "\n",
    "# Window length between training samples: number of days\n",
    "sample_window = 30\n",
    "\n",
    "# test period start\n",
    "test_start_date = '2020-06-30'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**We construct data_dict and label_dict for our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_er_ari(filepath, label_window=pm_window):\n",
    "    er_ari_df = pd.read_parquet(filepath)\n",
    "    \n",
    "\n",
    "    data_dict = {ticker: er_ari_df[ticker].dropna() for ticker in er_ari_df.columns}\n",
    "    tickers_to_remove = []\n",
    "    \n",
    "    label_dict = {}\n",
    "    for ticker, series in tqdm(data_dict.items()):\n",
    "        if series.isna().sum() == series.shape[0]:\n",
    "            tickers_to_remove += [ticker]\n",
    "            continue\n",
    "\n",
    "        last_date = series.index[-1] - relativedelta(years=pm_window)\n",
    "        if last_date <= series.index[0]:\n",
    "            tickers_to_remove.append(ticker)\n",
    "            continue\n",
    "\n",
    "        index = series.loc[:series.index[-1] - relativedelta(years=pm_window)].index\n",
    "        label_dict[ticker] = pd.Series([\n",
    "            series[date + relativedelta(years=pm_window)] for date in index\n",
    "        ], index=index)\n",
    "        \n",
    "    _ = [data_dict.pop(ticker) for ticker in tickers_to_remove]\n",
    "    \n",
    "    return data_dict, label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1330/1330 [03:33<00:00,  6.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dict, label_dict = prepare_data_for_er_ari(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_dict=pd.read_csv(\"data_dict.csv\")\\ndata_dict[\"Date\"]=data_dict[\"Date\"].astype(\\'datetime64[ns]\\')\\nlabel_dict=pd.read_csv(\"label_dict.csv\")\\nlabel_dict[\"Date\"]=label_dict[\"Date\"].astype(\\'datetime64[ns]\\')\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_dict=pd.read_csv(\"data_dict.csv\")\n",
    "data_dict[\"Date\"]=data_dict[\"Date\"].astype('datetime64[ns]')\n",
    "label_dict=pd.read_csv(\"label_dict.csv\")\n",
    "label_dict[\"Date\"]=label_dict[\"Date\"].astype('datetime64[ns]')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(data_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We construct the initial train_data and test_data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1130/1130 [00:09<00:00, 114.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "# test start date\n",
    "checkpoint = datetime.strptime(test_start_date, '%Y-%m-%d') - relativedelta(years=pm_window)\n",
    "\n",
    "for ticker in tqdm(tickers):    \n",
    "    label = label_dict[ticker]\n",
    "    if label.shape[0] == 0:\n",
    "        continue\n",
    "    ts = data_dict[ticker].loc[:label.index[-1]]\n",
    "\n",
    "    indices = [np.arange(i, i+lb_window, feat_window) for i in range(0, ts.shape[0] - lb_window + 1, sample_window)]\n",
    "    \n",
    "    temp_data = np.array([ts.iloc[sub_indices].values for sub_indices in indices])\n",
    "    if temp_data.shape[0] == 0:\n",
    "        continue\n",
    "    temp_labels = np.array([label.loc[ts.index[sub_indices[-1]]] for sub_indices in indices])\n",
    "    \n",
    "    train_indices = [idx for idx in range(temp_data.shape[0]) if ts.index[indices[idx][-1]] <= checkpoint]\n",
    "    test_indices = [idx for idx in range(temp_data.shape[0]) if ts.index[indices[idx][-1]] > checkpoint]\n",
    "    \n",
    "    train_data += [temp_data[train_indices]] \n",
    "    train_labels += [temp_labels[train_indices]]\n",
    "    \n",
    "    test_data += [temp_data[test_indices]] \n",
    "    test_labels += [temp_labels[test_indices]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By concatenation, we build our completed version of train_data and test_data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "test_data = np.concatenate(test_data)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moreover, we also need validation set split from train_data to validate the model derived by training set \n",
    "(which is the complement of validation set in train_data) to better serve model adjustment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, train_size=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next,  introduce the search algorithm used for this project.\n",
    "\n",
    "Generally, there are two simple and well-known algorithms used to optimize decision variables, namely, in our security selection project case, to search for optimal hyper-parameters: \n",
    "\n",
    "1. **Grid Search**:\n",
    "\n",
    "    We first divide our decision variable into different segments\n",
    "    \n",
    "    We try out a value in each segment and compute the objective value\n",
    "    \n",
    "    Then, choose the best one to optimize the objective function\n",
    "    \n",
    "    \n",
    "2. **Random Search**:\n",
    "\n",
    "    Randomly sample a value from the whole space and compute the objective value\n",
    "    \n",
    "    Repeat the above step for N times\n",
    "    \n",
    "    Then choose the best value to optimize the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search and grid search both serve as good starting points for HPO. They are not only very simple to be implemented, but also could be run in parallel to verify each other.\n",
    "\n",
    "However, both two have significant drawbacks: First and foremost, they do not learn from previous experience so **there is no guarantee of finding a local minimum to some precision unless the search space is thoroughly sampled**. Second, **they are extremely expensive search algorithms for time series models which deals with large and complex datasets**. \n",
    "\n",
    "**So we want to find a method that could not only learn from its past trials but also is time-efficient and computer-resouce-efficient, so that it can better deal with the model that is computationally expensive together with more hyperparameters.** \n",
    "\n",
    "So here we introduce one of the most efficient and cutting-edge methods, which is called **Bayesian Optimization (BO)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO Algorithm:\n",
    "\n",
    "Bayesian optimization is a technique for solving optimization problems where the objective function does not have an analytic expression, rather it can only be evaluated through some time-consuming operation. The powerful point for BO is **even if the true objective function is unknown, we can still fit a model to the observations we have so far and use the model to predict good parts of the parameter space where we should run additional experiments.**\n",
    "\n",
    "Bayesian optimization works by **constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize**. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below. It could be viewed as an updated version of TPE algorithm. Here is a simple demonstration of how it works: We sample x from its space and compute the corresponding objective function value y. \n",
    "\n",
    "![alt text](bo_example.png \"Title\")\n",
    "\n",
    "\n",
    "As you iterate over and over, the algorithm **balances its needs of exploration and exploitation taking into account what it knows about the target function**. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored (see the gif below).\n",
    "Once we get enough samples, the model will then split the outcomes using y∗ as the cutoff point. \n",
    "\n",
    "![SegmentLocal](bayesian_optimization.gif \"segment\")\n",
    "\n",
    "This process is designed to **minimize the number of steps required to find a combination of parameters that are close to the optimal combination**. To do so, this method uses a proxy optimization problem (finding the maximum of the acquisition function) that, albeit still a hard problem, is **cheaper (in the computational sense) and common tools** can be employed. \n",
    "\n",
    "**Therefore Bayesian Optimization is most adequate for situations where sampling the function to be optimized is a very expensive endeavor.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the background of the BO algorithm and its strengths compared with random or grid search, we use the bayes_opt, **a powerful Python library that can perform the BO algorithm to realize hyperparameter optimization.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The key hyperparameters:\n",
    "\n",
    "### 1. Learning rate\n",
    "\n",
    "    Learning rate controls how quickly or slowly a neural network model learns a problem. The amount that the weights are updated during training is denoted as the learning rate. Learning rate is a small positive value, generally in the range [0,1].\n",
    "    \n",
    "    Intuition: Small learning rate can lead to long time for converging while large learning rate can even lead to divergent situation.\n",
    "    \n",
    "### 2. Epochs\n",
    "\n",
    "    It defines the number of times that the learning algorithm will pass through the entire training dataset.\n",
    "    \n",
    "    Intuition: one epoch means each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "    \n",
    "### 3. Batch Size\n",
    "\n",
    "    It controls how often to update the weights of the network.\n",
    "\n",
    "\n",
    "After an understanding of key hyperparameters in the LSTM model and discussions with TAs, we believe “learning rate” and \"batch size\" are the most critical hyperparameters that could impact the performance of model prediction. Therefore, with this assumption, the main work we have been doing for the past several months is to use BO to search for a better learning rate and batch size, with epochs fixed."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83a8f8fdd9a4bfee2adbfced20061e3a8205137c55f327fc17073af9bf339e94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
